
# VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers
<div align="center">


**ICCV 2025**

[**Project Page**](https://xiaoxiao0406.github.io/vqvla.github.io/) | [**Paper**](https://xiaoxiao0406.github.io/vqvla.github.io/static/pdfs/VQ-VLA.pdf) | [**arXiv**](https://arxiv.org/pdf/2507.01016) 

[Yating Wang](https://scholar.google.com/citations?hl=zh-CN&user=5SuBWh0AAAAJ), [Haoyi Zhu](https://www.haoyizhu.site/), [Mingyu Liu](https://mingyulau.github.io/), [Jiange Yang](https://yangjiangeyjg.github.io/),  [Hao-Shu Fang](https://fang-haoshu.github.io/), [Tong He](http://tonghe90.github.io/)
<!-- <hr style="border: 2px solid gray;"></hr> -->
</div>

![teaser](assets/vqvla_pipeline.png)

**VQ-VLA** is an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. It demonstrates that action tokenizers can be effectively scaled by leveraging large-scale simulated action data. We prove that our action tokenizers improve the performance, inference speed, and long-horizon capabilities of
VLA models.

## Code is still under preparation. All code will be available later.

#### Citation

```bibtex
@article{wang25vqvla,
    title={VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers},
    author={Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, Tong He},
    journal = {iccv2025},
    year={2025}
}
```
